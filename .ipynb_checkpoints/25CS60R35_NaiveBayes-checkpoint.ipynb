{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b49ddb",
   "metadata": {},
   "source": [
    "## Assignment-2 : Naive Bayes\n",
    "### Name: PARTH UPADHYAY\n",
    "### Roll: 25CS60R35\n",
    "### Section: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1f3e4",
   "metadata": {},
   "source": [
    "## Set-up and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a10fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (2.3.4)\n",
      "Requirement already satisfied: matplotlib in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: seaborn in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from seaborn) (2.3.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: ucimlrepo in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from ucimlrepo) (2.3.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from ucimlrepo) (2025.10.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/parth/Downloads/yes/envs/my-conda-env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Installation requirements\n",
    "!pip install scikit-learn pandas numpy matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# To fetch datasets from UCI repository\n",
    "!pip install ucimlrepo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634ad2a-0246-458a-a270-f46831a72343",
   "metadata": {},
   "source": [
    "## Spambase Dataset(Odd Roll Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778d84f-c3a6-449d-be6a-85348bb90ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "  \n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbe5d7-4f9c-4467-a26a-379fe0c34c6d",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eaa8a2-6465-4634-85c5-bec785bac3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train-test splits(80%-20%) and handle missing values\n",
    "# Your Code Here\n",
    "X.isnull().sum().sum(),y.isnull().sum().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e0d20-8b29-47ad-9f98-f86afdf24071",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18699c-1116-437a-83d6-cb86b7d48051",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a927a28-b253-48e1-8d7d-606e8bebc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram plots/Bar Charts(count plots) for at least three features \n",
    "# Your Code Here\n",
    "df = pd.concat([X, y], axis=1)\n",
    "mi = mutual_info_classif(X, y.values.ravel())\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "features=['word_freq_free','char_freq_$', 'capital_run_length_total']\n",
    "top12 = [f for f in mi_series.index if f not in features][:12]\n",
    "\n",
    "features_to_plot = features + top12\n",
    "for value in features_to_plot:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(df[df['Class'] == 1][value], bins=30, alpha=0.4, label='Spam', color='red')\n",
    "    plt.hist(df[df['Class'] == 0][value], bins=30, alpha=0.4, label='Not Spam', color='blue')\n",
    "    plt.title(f\"Distribution of Class ( {value})\")\n",
    "    plt.xlabel(value)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fab01-c4e3-4686-aefe-f70439e6612d",
   "metadata": {},
   "source": [
    "## Implement Naive Bayes from Scratch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b90e8-0c1a-46ef-8f85-a57eda06d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.mean = {}         \n",
    "        self.var_smooth = {}   \n",
    "        self.class_priors = {} \n",
    "        self.labels = None\n",
    "    # calculate Gaussian probability\n",
    "    def gaussian_prob(self, mean, var_smooth, x_new):\n",
    "        exponent = -0.5 * ((x_new - mean) ** 2) / var_smooth\n",
    "        coeff = 1.0 / np.sqrt(2 * np.pi * var_smooth)\n",
    "        return coeff * np.exp(exponent)\n",
    "    # find mean, variance, and class priors \n",
    "    def fit(self, X, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.iloc[:, 0] \n",
    "        self.labels = np.unique(y)\n",
    "        for label in self.labels:\n",
    "            X_label = X[y == label]\n",
    "            self.mean[label] = X_label.mean().values\n",
    "            self.var_smooth[label] = X_label.var().values + self.alpha\n",
    "            self.class_priors[label] = len(X_label) / len(X)\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        X_values = X.values  \n",
    "        for x_new in X_values:\n",
    "            class_probs = {}\n",
    "            for label in self.labels:\n",
    "                log_prob = np.log(self.class_priors[label])\n",
    "                probs = self.gaussian_prob(self.mean[label], self.var_smooth[label], x_new)\n",
    "                probs = np.clip(probs, 1e-30, None) \n",
    "                log_prob += np.sum(np.log(probs))\n",
    "                class_probs[label] = log_prob\n",
    "            \n",
    "            y_pred.append(max(class_probs, key=class_probs.get))\n",
    "        \n",
    "        return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ad85c-a754-41cd-9fab-fb9d327d2aa9",
   "metadata": {},
   "source": [
    "## Visualization and Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e53ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the y values for the test set using the best model\n",
    "# Your Code Here\n",
    "\n",
    "# Experiment with different hyperparameter values\n",
    "alphas = [0.0001,0.001,0.01,0.1,1,10]\n",
    "# Your Code Here\n",
    "best_accu=0\n",
    "for alpha in alphas:\n",
    "    nb = NaiveBayesClassifier(alpha)\n",
    "    nb.fit(x_train, y_train)\n",
    "    y_pred=nb.predict(x_test)\n",
    "    y_true = np.array(y_test).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    accuracy=np.sum(y_true==y_pred)\n",
    "    if(accuracy > best_accu):\n",
    "        best_accu=accuracy\n",
    "        best_alpha=alpha\n",
    "    print(f\"accuracy of  {alpha} :  {accuracy / len(y_true)*100:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a00e8-dfc7-4a51-8eb0-f6f7ad8b4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01216c2a-3272-4ab5-9b97-ce88289e44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    \n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "# Use the best alpha model\n",
    "nb_best = NaiveBayesClassifier(best_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75025d64-11a6-4a6c-8737-e51d448844aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Data')\n",
    "nb_best.fit(x_train, y_train)\n",
    "y_pred_best = nb_best.predict(x_test)\n",
    "TP, FP, FN, TN = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"TP: {TP}, FP: {FP}\")\n",
    "print(f\"FN: {FN}, TN: {TN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18161055-273a-43be-9ae3-d0d992c3660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Data')\n",
    "nb_best.fit(x_train, y_train)\n",
    "y_pred_best = nb_best.predict(x_train)\n",
    "TP, FP, FN, TN = confusion_matrix(y_train, y_pred_best)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"TP: {TP}, FP: {FP}\")\n",
    "print(f\"FN: {FN}, TN: {TN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy, weighted precision, recall, F1 score manually\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    labels = np.unique(y_true)\n",
    "    \n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    \n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    support_sum = 0\n",
    "    \n",
    "    for label in labels:\n",
    "        TP = np.sum((y_pred == label) & (y_true == label))\n",
    "        FP = np.sum((y_pred == label) & (y_true != label))\n",
    "        FN = np.sum((y_pred != label) & (y_true == label))\n",
    "        support = np.sum(y_true == label)\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        \n",
    "        precision_sum += precision * support\n",
    "        recall_sum += recall * support\n",
    "        support_sum += support\n",
    "    \n",
    "    precision_weighted = precision_sum / support_sum\n",
    "    recall_weighted = recall_sum / support_sum\n",
    "    f1_weighted = 2 * (precision_weighted * recall_weighted) / (precision_weighted + recall_weighted) \\\n",
    "                  if (precision_weighted + recall_weighted) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision_weighted, recall_weighted, f1_weighted\n",
    "\n",
    "\n",
    "def analysis_train_test(X_train, y_train, X_test, y_test, alphas):\n",
    "    metrics_train = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    metrics_test  = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "    for alp in alphas:\n",
    "        nb = NaiveBayesClassifier(alpha=alp)\n",
    "        nb.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = nb.predict(X_train)\n",
    "        y_test_pred  = nb.predict(X_test)\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc_tr, prec_tr, rec_tr, f1_tr = compute_metrics(y_train, y_train_pred)\n",
    "        acc_te, prec_te, rec_te, f1_te = compute_metrics(y_test, y_test_pred)\n",
    "        \n",
    "        metrics_train['accuracy'].append(acc_tr)\n",
    "        metrics_train['precision'].append(prec_tr)\n",
    "        metrics_train['recall'].append(rec_tr)\n",
    "        metrics_train['f1'].append(f1_tr)\n",
    "        \n",
    "        metrics_test['accuracy'].append(acc_te)\n",
    "        metrics_test['precision'].append(prec_te)\n",
    "        metrics_test['recall'].append(rec_te)\n",
    "        metrics_test['f1'].append(f1_te)\n",
    "        \n",
    "        print(f\"Alpha : {alp}\")\n",
    "        print(f\"Train :\\n Accuracy: {acc_tr*100:.2f}%\\n Precision: {prec_tr*100:.2f}%\\n Recall: {rec_tr*100:.2f}%\\n F1: {f1_tr*100:.2f}%\")\n",
    "        print(f\"Test  :\\n Accuracy: {acc_te*100:.2f}%\\n Precision: {prec_te*100:.2f}%\\n Recall: {rec_te*100:.2f}%\\n F1: {f1_te*100:.2f}%\\n\")\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(alphas, metrics_train[metric], marker='o', label='Train')\n",
    "        plt.plot(alphas, metrics_test[metric], marker='s', label='Test')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Alpha (smoothing parameter) ')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f'{metric.capitalize()} vs Alpha')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4cb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a plot with alpha along the x-axis and training/test accuracy/recall/precision/F1 along the y-axis\n",
    "# Your Code Here\n",
    "\n",
    "analysis_train_test(x_train, y_train, x_test, y_test, alphas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffce3a0",
   "metadata": {},
   "source": [
    "#### Explain which value of ‚Äòa‚Äô is most suitable for your dataset and why. \n",
    "#### Discuss how this smoothing parameter influences the model and helps prevent overfitting.\n",
    "\n",
    "#### Best - alpha 0.001 \n",
    "Very small Œ± (0.0001) barely smooths the variance that leads to  model slightly overfits\\\n",
    "Moderate Œ± (0.001) provides enough smoothing to stabilize Gaussian estimates which improves generalization.\\\n",
    "Too large Œ± (0.1, 1, 10) overly inflates variance so features become less informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f04b1-3165-4737-a30d-cf3684b10c88",
   "metadata": {},
   "source": [
    "## Investigating the Independence Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce0f67-a356-4b96-9e48-5eb110a6d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "feature = 'char_freq_$'\n",
    "\n",
    "def create_feature_duplicates(df, feature, n):\n",
    "    df_new = df.copy()\n",
    "    for i in range(1, n+1):\n",
    "        df_new[f'copy{i}'] = df_new[feature]\n",
    "    return df_new\n",
    "df1 = create_feature_duplicates(df, feature, 1)\n",
    "df2 = create_feature_duplicates(df, feature, 2)\n",
    "df3 = create_feature_duplicates(df, feature, 3)\n",
    "df4 = create_feature_duplicates(df, feature, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01aabfe-5619-4173-b705-678b9a33ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_(df, best_alpha):\n",
    "    X = df.drop('Class', axis=1)\n",
    "    Y = df['Class']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    nb = NaiveBayesClassifier(best_alpha)\n",
    "    nb.fit(x_train, y_train)\n",
    "    y_test_pred = nb.predict(x_test)\n",
    "    y_true_test = np.array(y_test).flatten()\n",
    "    y_pred_test = np.array(y_test_pred).flatten()\n",
    "    accuracy_test = np.sum(y_true_test == y_pred_test) / len(y_true_test)\n",
    "    y_train_pred = nb.predict(x_train)\n",
    "    y_true_train = np.array(y_train).flatten()\n",
    "    y_pred_train = np.array(y_train_pred).flatten()\n",
    "    accuracy_train = np.sum(y_true_train == y_pred_train) / len(y_true_train)\n",
    "    return accuracy_test, accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1f55f-5c2c-43c2-a090-f43c22971e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a7266-e61f-422b-91fe-4e0df0e43e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Training and Evaluation using the best hyperparameter 'a'\n",
    "# Evaluate training/test accuracy/recall/precision/F1\n",
    "\n",
    "# Dataset 0(Zero copy)\n",
    "# Your Code Here\n",
    "print(\"0 copy used\")\n",
    "accuracy1,accuracy2=calc_(df,best_alpha)\n",
    "print(f\"Test accuracy : {accuracy1*100:.2f}%\")\n",
    "print(f\"Train accuracy : {accuracy2*100:.2f}%\")\n",
    "dic['0']={\n",
    "    'test':accuracy1,\n",
    "    'train':accuracy2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b835bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Training and Evaluation using the best hyperparameter 'a'\n",
    "# Evaluate training/test accuracy/recall/precision/F1\n",
    "\n",
    "# Dataset 1(one copy)\n",
    "# Your Code Here\n",
    "print(\"1 copy used\")\n",
    "accuracy1,accuracy2=calc_(df1,best_alpha)\n",
    "print(f\"Test accuracy : {accuracy1*100:.2f}%\")\n",
    "print(f\"Train accuracy : {accuracy2*100:.2f}%\")\n",
    "dic['1']={\n",
    "    'test':accuracy1,\n",
    "    'train':accuracy2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ccf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2(two copies)\n",
    "# Your Code Here\n",
    "print(\"2 copy used\")\n",
    "accuracy1,accuracy2=calc_(df2,best_alpha)\n",
    "print(f\"Test accuracy : {accuracy1*100:.2f}%\")\n",
    "print(f\"Train accuracy : {accuracy2*100:.2f}%\")\n",
    "dic['2']={\n",
    "    'test':accuracy1,\n",
    "    'train':accuracy2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461180b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 3(three copies)\n",
    "# Your Code Here\n",
    "print(\"3 copy used\")\n",
    "accuracy1,accuracy2=calc_(df3,best_alpha)\n",
    "print(f\"Test accuracy : {accuracy1*100:.2f}%\")\n",
    "print(f\"Train accuracy : {accuracy2*100:.2f}%\")\n",
    "dic['3']={\n",
    "    'test':accuracy1,\n",
    "    'train':accuracy2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 4(four copies)\n",
    "# Your Code Here\n",
    "print(\"4 copy used\")\n",
    "accuracy1,accuracy2=calc_(df4,best_alpha)\n",
    "print(f\"Test accuracy : {accuracy1*100:.2f}%\")\n",
    "print(f\"Train accuracy : {accuracy2*100:.2f}%\")\n",
    "dic['4']={\n",
    "    'test':accuracy1,\n",
    "    'train':accuracy2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4508fb0",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ceef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both the training/test accuracy/recall/precision/F1 as a function of the number of duplicate features added\n",
    "# The x-axis should be the \"Number of Added Copies\" (0, 1, 2, 3, 4), and the y-axis should show the accuracy/recall/precision/F1.\n",
    "# Your Code Here\n",
    "num_copies_str = sorted(dic.keys())\n",
    "num_copies = [int(k) for k in num_copies_str]\n",
    "test_accuracies = [dic[k]['test'] for k in num_copies_str]\n",
    "train_accuracies = [dic[k]['train'] for k in num_copies_str]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_copies, train_accuracies, marker='o', linestyle='-', label='Training Accuracy')\n",
    "plt.plot(num_copies, test_accuracies, marker='s', linestyle='--', label='Test Accuracy')\n",
    "\n",
    "plt.xlabel(\"Number of Added Copies\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Naive Bayes Accuracy vs. Number of Duplicate Features Added\")\n",
    "plt.xticks(num_copies) # Ensure ticks are at 0, 1, 2, 3, 4\n",
    "plt.ylim(min(min(test_accuracies), min(train_accuracies)) - 0.02, max(max(test_accuracies), max(train_accuracies)) + 0.02) # Adjust y-limits based on data\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e22d4b-0fa0-4411-88e5-09f3037a6695",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "#### How does adding duplicate (and thus perfectly correlated) features affect the classifier's performance?\n",
    "#### Explain this behavior by referencing the Naive Bayes decision rule.\n",
    "#### What happens mathematically to the likelihood term when you add a copy of a feature?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876394ec-c5fa-48cc-81e3-4a061d19c461",
   "metadata": {},
   "source": [
    "\n",
    "#### In this dataset, the duplicated feature is strongly predictive, so reinforcing it helps Naive Bayes a little.\n",
    "Qs 1)The model does not immediately overfit and confidence increases and accuracy rises.\n",
    "this is not guaranteed generally .\\  \n",
    "\n",
    "Qs 2)The Naive Bayes decision rule selects the class ùë¶ that maximizes:\n",
    "\n",
    "y^=argymax[logP(y)+‚àëlogP(xi‚à£y)] \n",
    "\n",
    "Naive Bayes assumes all features xi are independent given the class.\n",
    "\n",
    "Qs 3)\n",
    "When a feature ùë•ùëò is duplicated, say we add an identical copy xk , the model treats it as an additional independent predictor, so the likelihood becomes:\n",
    "logP(y)+[logP(xk‚à£y)+logP(xk‚Ä≤‚à£y)]+‚àëlogP(xi‚à£y)\n",
    "\n",
    "Since ùë•ùëò=ùë•ùëò we get,\n",
    "logP(y)+2logP(xk‚à£y)+i=k‚àëlogP(xi‚à£y)\n",
    "So mathematically, adding a copy doubles the contribution of that feature to the decision rule.\n",
    "In probability space (before taking logs), this means:\n",
    "P(xk‚à£y)^2\n",
    "Thus, Naive Bayes becomes overconfident in that feature, because it believes the same information twice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
